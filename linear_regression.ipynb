{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a000a813-154c-462f-82da-e9b8e8d1611b",
   "metadata": {},
   "source": [
    "Terlebih dahulu panggil pustaka yang akan kita gunakan. Kita akan memakai 3 pustaka, yakni:\n",
    "- **numpy** (Untuk melakukan proses komputasi vektor numerik pada dataset dan implementasi algoritme)\n",
    "- **matplotlib** (Untuk visualisasi proses berjalannya algoritme)\n",
    "- **sklearn** (Hanya digunakan untuk memanggil dataset yang akan kita proses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b8798-01f6-4f29-91a5-0da3a6f904b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773bd11-f5f9-4264-9523-ea436ce78c11",
   "metadata": {},
   "source": [
    "Sekarang panggil dataset regresi dengan perintah `load_boston`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b623e4e-cb0c-4c02-b0f0-ff81cd863a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d96eb3-9f1c-4953-ab68-cf7125d0b141",
   "metadata": {},
   "source": [
    "Selanjutnya kita pisah dataset yang sudah dipanggil di atas berdasarkan **feature** dan **label**. Feature akan kita masukkan variabel `X` dan label kita masukan ke variabel `y`. Ubah dimensi label dengan menambahkan perintah `[:,np.newaxis]`. Karena vektor kolom dan vektor baris berbeda dalam Aljabar Linear.\n",
    "\n",
    "Namun, di numpy hanya ada array n-dimensi dan tidak ada konsep untuk vektor baris dan kolom. Maka digunakan array bentuk *(n, 1)* untuk meniru vektor kolom dan *(1, n)* untuk vektor baris. Bentuk nilai label dapat menggunakan *(n, )* sebagai bentuk vektor kolom bentuk *(n, 1)* dengan menambahkan sumbu secara eksplisit, bila digunakan sekali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76addd-007f-4be8-b11b-7d9365f5cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target[:,np.newaxis]\n",
    "\n",
    "print(\"Total dataset yang digunakan adalah: {}\".format(X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c89d1-546e-4c39-9c34-5dbe589d3256",
   "metadata": {},
   "source": [
    "*Mean Squared Error (MSE)* akan diterapkan sebagai *cost function*. \n",
    "Variabel `h` untuk memasukkan nilai hasil pemetaan dari *input* `X` ke *output* `y`.\n",
    "\n",
    "*Inner product* dari *features* diambil dengan parameter `(X @ params)`, perintah tersebut secara eksplisit menyatakan bahwa \n",
    "regresi linier akan digunakan untuk memproses hipotesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e7e76-24a9-417e-9252-b17823cb5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, params):\n",
    "    n_samples = len(y)\n",
    "    h = X @ params\n",
    "    return (1/(2*n_samples))*np.sum((h-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c96c9-299d-456d-b0b3-8f7f981f048f",
   "metadata": {},
   "source": [
    "Selanjutnya algoritme *gradient descent* akan diimplementasikan. Parameter `n_inters` menunjukkan jumlah iterasi dari *gradient descent*. Nilai *cost* yang dihasilkan oleh *cost function* pada setiap iterasi akan disimpan pada variabel `J_history` yang sudah didefinisikan sebagai *numpy array*.\n",
    "\n",
    "*Update rule* ditentukan dengan *script* berikut:\n",
    "`(1/n_samples) * X.T @ (X @ params - y)`\n",
    "\n",
    "Sesudai dengan `partial derivative` dari `cost function`. Jadi, variabel `params` akan menyimpan nilai parameter yang diperbarui sesuai dengan aturan di atas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a4f88-1a8c-46e9-8068-45c150ed1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, params, learning_rate, n_iters):\n",
    "    n_samples = len(y)\n",
    "    J_history = np.zeros((n_iters,1))\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        params = params - (learning_rate/n_samples) * X.T @ (X @ params - y) \n",
    "        J_history[i] = compute_cost(X, y, params)\n",
    "\n",
    "    return (J_history, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f5ba9c-bed3-4a64-b2e6-683f453279e5",
   "metadata": {},
   "source": [
    "Sebelum menjalankan algoritme *gradient descent* pada *dataset*, terlebih dahulu terapkan normalisasi pada data.\n",
    "Normalisasi sering dilakukan sebagai bagian persiapan data di setiap proses *machine learning*. Kali ini normalisasi yang dilakukan yakni *rescaling* nilai pada rentang `[0,1]` demi meningkatkan akurasi sekaligus untuk menurunkan nilai *cost/error*. Selain itu nilai parameter pada variabel `params` diset menjadi *zeros*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f64e6a-d970-4730-8449-0ef156870190",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(y)\n",
    "\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "\n",
    "X = (X-mu) / sigma\n",
    "\n",
    "X = np.hstack((np.ones((n_samples,1)),X))\n",
    "n_features = np.size(X,1)\n",
    "params = np.zeros((n_features,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb64135-87f5-441d-87d5-5ec014038592",
   "metadata": {},
   "source": [
    "Yak! Algoritme sudah dijalankan, terlihat nilai `cost` menurun drastis dari 296 menjadi 11. Fungsi gradient_descent mengembalikan nilai parameter yang optimal. Nah sekarang nilai tersebut dapat digunakan untuk memprediksi nilai target baru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29173695-77e9-4b48-8ac1-cc3a294817fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 1500\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, y, params)\n",
    "\n",
    "print(\"Nilai cost awal adalah: \", initial_cost, \"\\n\")\n",
    "\n",
    "(J_history, optimal_params) = gradient_descent(X, y, params, learning_rate, n_iters)\n",
    "\n",
    "print(\"Data parameter yang optimal: \\n\", optimal_params, \"\\n\")\n",
    "\n",
    "print(\"Nilai cost akhir: \", J_history[-1])\n",
    "\n",
    "plt.plot(range(len(J_history)), J_history, 'r')\n",
    "\n",
    "plt.title(\"Grafik Konvergensi Cost Function\")\n",
    "plt.xlabel(\"Jumlah Iterasi\")\n",
    "plt.ylabel(\"Nilai Cost\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_linear_regression",
   "language": "python",
   "name": "scratch_linear_regression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}